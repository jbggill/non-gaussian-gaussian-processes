{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c1aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessegill/Desktop/nggp/nggp_lib/reporting/loggers.py:1: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  import neptune.new as neptune\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from methods.regression_methods import NGGP, get_transforms\n",
    "from run_regression import *\n",
    "from training.configs import Config as config\n",
    "from models import backbone\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "from data.neural_loader import *\n",
    "from torch import tensor\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa0a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_th = '/Users/jessegill/Desktop/nggp/nggp_lib/save/nggp_rbf_5e3/checkpoints/neural/MLP2_NGGP_model.th'\n",
    "data = torch.load(save_path_th)\n",
    "params = Namespace(seed=1, model='MLP2', method='NGGP', dataset='neural', update_batch_size=5, meta_batch_size=5, output_dim=40, multidimensional_amp=False, multidimensional_phase=False, noise='gaussian', kernel_type='rbf', save_dir='./save/nggp_rbf_5e3', num_tasks=1, multi_type=3, method_lr=0.001, feature_extractor_lr=0.001, cnf_lr=0.001, all_lr=0.005, neptune=False, use_conditional=False, context_type='backbone', layer_type='concatsquash', dims='32-32', num_blocks=2, time_length=0.5, train_T=False, add_noise=False, divergence_fn='brute_force', nonlinearity='tanh', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, start_epoch=0, stop_epoch=100, test=False, n_support=5, n_test_epochs=10, out_of_range=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d84b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_th = '/Users/jessegill/Desktop/nggp/nggp_lib/save/sines_1e6/checkpoints/sines/MLP2_NGGP_model.th'\n",
    "params = Namespace(seed=1, model='MLP2', method='NGGP', dataset='sines', update_batch_size=5, meta_batch_size=5, output_dim=40, multidimensional_amp=False, multidimensional_phase=False, noise='gaussian', kernel_type='rbf', save_dir='./save/sines_1e6', num_tasks=1, multi_type=3, method_lr=0.001, feature_extractor_lr=0.001, cnf_lr=0.001, all_lr=1e-06, neptune=False, use_conditional=True, context_type='backbone', layer_type='concatsquash', dims='32-32', num_blocks=2, time_length=0.5, train_T=False, add_noise=False, divergence_fn='brute_force', nonlinearity='tanh', solver='dopri5', atol=1e-05, rtol=1e-05, step_size=None, test_solver=None, test_atol=None, test_rtol=None, residual=False, rademacher=False, spectral_norm=False, batch_norm=False, bn_lag=0, l1int=None, l2int=None, dl2int=None, JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, start_epoch=0, stop_epoch=100, test=False, n_support=5, n_test_epochs=10, out_of_range=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378c1b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./save/sines_1e6/checkpoints/sines/MLP2_NGGP_model.th\n",
      "[0] - Loss: 1.770  MSE: 3.092 noise: 0.693\n",
      "[0] - Loss: 2.708  MSE: 5.847 noise: 0.693\n",
      "[0] - Loss: 1.004  MSE: 0.011 noise: 0.693\n",
      "[0] - Loss: 1.315  MSE: 1.477 noise: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessegill/Desktop/nggp/nggp_lib/CNF_lib/layers/odefunc.py:350: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - Loss: 1.123  MSE: 0.405 noise: 0.693\n",
      "[10] - Loss: 1.253  MSE: 0.866 noise: 0.693\n",
      "[10] - Loss: 4.406  MSE: 12.606 noise: 0.693\n",
      "[10] - Loss: 1.089  MSE: 0.274 noise: 0.693\n",
      "[10] - Loss: 1.139  MSE: 0.543 noise: 0.693\n",
      "[10] - Loss: 4.063  MSE: 13.479 noise: 0.693\n",
      "[20] - Loss: 3.613  MSE: 11.057 noise: 0.693\n",
      "[20] - Loss: 1.112  MSE: 0.340 noise: 0.693\n",
      "[20] - Loss: 2.118  MSE: 4.703 noise: 0.693\n",
      "[20] - Loss: 2.088  MSE: 3.984 noise: 0.693\n",
      "[20] - Loss: 1.394  MSE: 1.380 noise: 0.693\n",
      "[30] - Loss: 2.297  MSE: 5.899 noise: 0.693\n",
      "[30] - Loss: 2.342  MSE: 4.250 noise: 0.693\n",
      "[30] - Loss: 1.808  MSE: 3.307 noise: 0.693\n",
      "[30] - Loss: 1.018  MSE: 0.110 noise: 0.693\n",
      "[30] - Loss: 1.518  MSE: 2.338 noise: 0.693\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     test(nggp_model, params, save_path, results_logger)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m run:\n\u001b[0;32m---> 19\u001b[0m     train(nggp_model, optimizer, params, save_path, results_logger)\n\u001b[1;32m     20\u001b[0m setup_checkpoint_dir(params)\n\u001b[1;32m     21\u001b[0m nggp_model\u001b[38;5;241m.\u001b[39mload_checkpoint(save_path_th, device)\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/run_regression.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, params, save_path, results_logger)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, optimizer, params, save_path, results_logger):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mstop_epoch):\n\u001b[0;32m---> 43\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain_loop(epoch, optimizer, params, results_logger)\n\u001b[1;32m     44\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_checkpoint(save_path)\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/methods/regression_methods.py:163\u001b[0m, in \u001b[0;36mNGGP.train_loop\u001b[0;34m(self, epoch, optimizer, params, results_logger)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_loop(epoch, optimizer, params, results_logger)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_loop(epoch, optimizer, params, results_logger)\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/methods/regression_methods.py:270\u001b[0m, in \u001b[0;36mNGGP._train_loop\u001b[0;34m(self, epoch, optimizer, params, results_logger)\u001b[0m\n\u001b[1;32m    267\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    268\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 270\u001b[0m mse, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_mse(labels, predictions, z)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] - Loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m  MSE: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m noise: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    273\u001b[0m         epoch, loss\u001b[38;5;241m.\u001b[39mitem(), mse\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39mnoise\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    275\u001b[0m     ))\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/methods/regression_methods.py:292\u001b[0m, in \u001b[0;36mNGGP.compute_mse\u001b[0;34m(self, labels, predictions, z)\u001b[0m\n\u001b[1;32m    290\u001b[0m     means \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_conditional:\n\u001b[0;32m--> 292\u001b[0m     new_means \u001b[38;5;241m=\u001b[39m sample_fn(means, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context(z))\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     new_means \u001b[38;5;241m=\u001b[39m sample_fn(means)\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/methods/regression_methods.py:23\u001b[0m, in \u001b[0;36mget_transforms.<locals>.sample_fn\u001b[0;34m(z, context, logpz)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(z, context, logpz, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(z, context, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/CNF_lib/layers/container.py:46\u001b[0m, in \u001b[0;36mSequentialFlowC.forward\u001b[0;34m(self, x, context, logpx, reverse, inds)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logpx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[0;32m---> 46\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain[i](x, context, reverse\u001b[38;5;241m=\u001b[39mreverse)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/CNF_lib/layers/cnf.py:137\u001b[0m, in \u001b[0;36mCNFC.forward\u001b[0;34m(self, z, context, logpz, integration_times, reverse)\u001b[0m\n\u001b[1;32m    135\u001b[0m     states \u001b[38;5;241m=\u001b[39m (z, _logpz, context)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 137\u001b[0m     state_t \u001b[38;5;241m=\u001b[39m odeint(\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modefunc,\n\u001b[1;32m    139\u001b[0m         states \u001b[38;5;241m+\u001b[39m reg_states,\n\u001b[1;32m    140\u001b[0m         integration_times\u001b[38;5;241m.\u001b[39mto(z),\n\u001b[1;32m    141\u001b[0m         atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol,\n\u001b[1;32m    142\u001b[0m         rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol,\n\u001b[1;32m    143\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver,\n\u001b[1;32m    144\u001b[0m         options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver_options,\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     state_t \u001b[38;5;241m=\u001b[39m odeint(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39modefunc,\n\u001b[1;32m    149\u001b[0m         states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_solver,\n\u001b[1;32m    154\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/adjoint.py:198\u001b[0m, in \u001b[0;36modeint_adjoint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[1;32m    195\u001b[0m state_norm \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    196\u001b[0m handle_adjoint_norm_(adjoint_options, shapes, state_norm)\n\u001b[0;32m--> 198\u001b[0m ans \u001b[38;5;241m=\u001b[39m OdeintAdjointMethod\u001b[38;5;241m.\u001b[39mapply(shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol,\n\u001b[1;32m    199\u001b[0m                                 adjoint_method, adjoint_options, t\u001b[38;5;241m.\u001b[39mrequires_grad, \u001b[38;5;241m*\u001b[39madjoint_params)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     solution \u001b[38;5;241m=\u001b[39m ans\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/adjoint.py:25\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.forward\u001b[0;34m(ctx, shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, t_requires_grad, *adjoint_params)\u001b[0m\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39mevent_mode \u001b[38;5;241m=\u001b[39m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     ans \u001b[38;5;241m=\u001b[39m odeint(func, y0, t, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, method\u001b[38;5;241m=\u001b[39mmethod, options\u001b[38;5;241m=\u001b[39moptions, event_fn\u001b[38;5;241m=\u001b[39mevent_fn)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         y \u001b[38;5;241m=\u001b[39m ans\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate(t)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/solvers.py:30\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_integrate(t)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[0;32m---> 30\u001b[0m     solution[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_advance(t[i])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solution\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:194\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_t \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt1:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m n_steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_steps, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_num_steps exceeded (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_steps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_steps)\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adaptive_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state)\n\u001b[1;32m    195\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _interp_evaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39minterp_coeff, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrk_state\u001b[38;5;241m.\u001b[39mt1, next_t)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:255\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    250\u001b[0m         dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Must be arranged as doing all the step_t handling, then all the jump_t handling, in case we\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# trigger both. (i.e. interleaving them would be wrong.)\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m y1, f1, y1_error, k \u001b[38;5;241m=\u001b[39m _runge_kutta_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, y0, f0, t0, dt, t1, tableau\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtableau)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# dtypes:\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# y1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# f1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#                     Error Ratio                      #\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m########################################################\u001b[39;00m\n\u001b[1;32m    265\u001b[0m error_ratio \u001b[38;5;241m=\u001b[39m _compute_error_ratio(y1_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol, y0, y1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:76\u001b[0m, in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m         perturb \u001b[38;5;241m=\u001b[39m Perturb\u001b[38;5;241m.\u001b[39mNONE\n\u001b[1;32m     75\u001b[0m     yi \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(k[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (beta_i \u001b[38;5;241m*\u001b[39m dt), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview_as(f0)\n\u001b[0;32m---> 76\u001b[0m     f \u001b[38;5;241m=\u001b[39m func(ti, yi, perturb\u001b[38;5;241m=\u001b[39mperturb)\n\u001b[1;32m     77\u001b[0m     k \u001b[38;5;241m=\u001b[39m _UncheckedAssign\u001b[38;5;241m.\u001b[39mapply(k, f, (\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tableau\u001b[38;5;241m.\u001b[39mc_sol[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (tableau\u001b[38;5;241m.\u001b[39mc_sol[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tableau\u001b[38;5;241m.\u001b[39mbeta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mall()):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# This property (true for Dormand-Prince) lets us save a few FLOPs.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_func(t, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_func(t, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:159\u001b[0m, in \u001b[0;36m_ReverseFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_func(\u001b[38;5;241m-\u001b[39mt, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:138\u001b[0m, in \u001b[0;36m_TupleFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 138\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_func(t, _flat_to_shape(y, (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapes))\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([f_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/CNF_lib/layers/odefunc.py:378\u001b[0m, in \u001b[0;36mODEfuncC.forward\u001b[0;34m(self, t, states)\u001b[0m\n\u001b[1;32m    376\u001b[0m         divergence \u001b[38;5;241m=\u001b[39m divergence_bf(dy, y)\u001b[38;5;241m.\u001b[39mview(batchsize, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m         divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivergence_fn(dy, y, e\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e)\u001b[38;5;241m.\u001b[39mview(batchsize, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m    380\u001b[0m     dy \u001b[38;5;241m=\u001b[39m dy \u001b[38;5;241m-\u001b[39m y\n",
      "File \u001b[0;32m~/Desktop/nggp/nggp_lib/CNF_lib/layers/odefunc.py:18\u001b[0m, in \u001b[0;36mdivergence_bf\u001b[0;34m(dx, y, **unused_kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m sum_diag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 18\u001b[0m     sum_diag \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(dx[:, i]\u001b[38;5;241m.\u001b[39msum(), y, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()[:, i]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sum_diag\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = True\n",
    "setup_seed(params)\n",
    "config = Config(params)\n",
    "checkpoint_dir, save_path = setup_checkpoint_dir(params)\n",
    "\n",
    "results_logger = ResultsLogger(params)\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "logging.info('Device: {}'.format(device))\n",
    "\n",
    "bb = setup_backbone(device, params)\n",
    "nggp_model = setup_model(bb, config, device, params)\n",
    "optimizer = setup_optimizer(nggp_model, params)\n",
    "\n",
    "if params.test and run:\n",
    "    test(nggp_model, params, save_path, results_logger)\n",
    "elif run:\n",
    "    train(nggp_model, optimizer, params, save_path, results_logger)\n",
    "setup_checkpoint_dir(params)\n",
    "nggp_model.load_checkpoint(save_path_th, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7e455e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1.7936,   1.8352,   1.8769,  ..., 896.4425, 896.4841, 896.5258])\n",
      "tensor(1.7936) tensor(-0.1400)\n",
      "tensor([  1.7936,   1.8352,   1.8769,  ..., 896.4425, 896.4841, 896.5258])\n",
      "tensor(1.7936) tensor(-0.1400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.7935729 , -0.14002801],\n",
       "       [ 1.8352395 , -0.14002801],\n",
       "       [ 1.8769062 , -0.32254297],\n",
       "       [ 1.9185729 , -0.3077935 ],\n",
       "       [ 1.9602395 , -0.39923096],\n",
       "       [ 2.0019062 , -0.42640144],\n",
       "       [ 2.043573  , -0.4818999 ],\n",
       "       [ 2.0852396 , -0.4618802 ],\n",
       "       [ 2.1269062 , -0.49656352],\n",
       "       [ 2.168573  , -0.31351885],\n",
       "       [ 2.2102396 , -0.39052093],\n",
       "       [ 2.2519062 , -0.34391797],\n",
       "       [ 2.293573  , -0.28867513],\n",
       "       [ 2.3352396 , -0.41053542],\n",
       "       [ 2.3769062 , -0.28867513],\n",
       "       [ 2.418573  , -0.34391797],\n",
       "       [ 2.4602396 , -0.34197238],\n",
       "       [ 2.5019062 , -0.24743582],\n",
       "       [ 2.543573  , -0.40616432],\n",
       "       [ 2.5852396 , -0.39904344],\n",
       "       [ 2.6269062 , -0.36115757],\n",
       "       [ 2.668573  , -0.39801487],\n",
       "       [ 2.7102396 , -0.31351885],\n",
       "       [ 2.7519062 , -0.37796447],\n",
       "       [ 2.793573  , -0.35846618],\n",
       "       [ 2.8352396 , -0.37463433],\n",
       "       [ 2.8769062 , -0.3077935 ],\n",
       "       [ 2.918573  , -0.3077935 ],\n",
       "       [ 2.9602396 , -0.38186362],\n",
       "       [ 3.0019062 , -0.32616404],\n",
       "       [ 3.043573  , -0.34391797],\n",
       "       [ 3.0852396 , -0.3944053 ],\n",
       "       [ 3.1269062 , -0.24743582],\n",
       "       [ 3.168573  , -0.26863533],\n",
       "       [ 3.2102396 , -0.41389614],\n",
       "       [ 3.2519062 , -0.24743582],\n",
       "       [ 3.293573  , -0.49046576],\n",
       "       [ 3.3352396 , -0.56458676],\n",
       "       [ 3.3769062 , -0.24743582],\n",
       "       [ 3.418573  , -0.40616432],\n",
       "       [ 3.4602396 , -0.49046576],\n",
       "       [ 3.5019062 , -0.37796447],\n",
       "       [ 3.543573  , -0.45425677],\n",
       "       [ 3.5852396 , -0.47543445],\n",
       "       [ 3.6269062 , -0.42640144],\n",
       "       [ 3.668573  , -0.54814637],\n",
       "       [ 3.7102396 , -0.54303646],\n",
       "       [ 3.7519062 , -0.24743582],\n",
       "       [ 3.793573  , -0.47543445],\n",
       "       [ 3.8352396 , -0.5350731 ],\n",
       "       [ 3.8769062 , -0.35846618],\n",
       "       [ 3.918573  , -0.45749572],\n",
       "       [ 3.9602396 , -0.41666666],\n",
       "       [ 4.0019064 , -0.34391797],\n",
       "       [ 4.043573  , -0.3840932 ],\n",
       "       [ 4.0852394 , -0.49046576],\n",
       "       [ 4.1269064 , -0.34391797],\n",
       "       [ 4.168573  , -0.47543445],\n",
       "       [ 4.2102394 , -0.44204333],\n",
       "       [ 4.2519064 , -0.41666666],\n",
       "       [ 4.293573  , -0.28867513],\n",
       "       [ 4.3352394 , -0.37070572],\n",
       "       [ 4.3769064 , -0.41053542],\n",
       "       [ 4.418573  , -0.2       ],\n",
       "       [ 4.4602394 , -0.49046576],\n",
       "       [ 4.5019064 , -0.37796447],\n",
       "       [ 4.543573  , -0.3077935 ],\n",
       "       [ 4.5852394 , -0.44204333],\n",
       "       [ 4.6269064 , -0.34391797],\n",
       "       [ 4.668573  , -0.26863533],\n",
       "       [ 4.7102394 , -0.38186362],\n",
       "       [ 4.7519064 , -0.5551361 ],\n",
       "       [ 4.793573  , -0.2       ],\n",
       "       [ 4.8352394 , -0.41389614],\n",
       "       [ 4.8769064 , -0.35846618],\n",
       "       [ 4.918573  , -0.3077935 ],\n",
       "       [ 4.9602394 , -0.4602873 ],\n",
       "       [ 5.0019064 , -0.34391797],\n",
       "       [ 5.043573  , -0.2247333 ],\n",
       "       [ 5.0852394 , -0.3944053 ],\n",
       "       [ 5.1269064 , -0.505403  ],\n",
       "       [ 5.168573  , -0.36115757],\n",
       "       [ 5.2102394 , -0.44500026],\n",
       "       [ 5.2519064 , -0.40616432],\n",
       "       [ 5.293573  , -0.37796447],\n",
       "       [ 5.3352394 , -0.48178765],\n",
       "       [ 5.3769064 , -0.36115757],\n",
       "       [ 5.418573  , -0.14002801],\n",
       "       [ 5.4602394 , -0.6088312 ],\n",
       "       [ 5.5019064 , -0.41053542],\n",
       "       [ 5.543573  , -0.2247333 ],\n",
       "       [ 5.5852394 , -0.51126987],\n",
       "       [ 5.6269064 , -0.47278896],\n",
       "       [ 5.668573  , -0.24743582],\n",
       "       [ 5.7102394 , -0.4602873 ],\n",
       "       [ 5.7519064 , -0.33129457],\n",
       "       [ 5.793573  , -0.2       ],\n",
       "       [ 5.8352394 , -0.49746835],\n",
       "       [ 5.8769064 , -0.28867513],\n",
       "       [ 5.918573  , -0.1723455 ],\n",
       "       [ 5.9602394 , -0.42954656],\n",
       "       [ 6.0019064 , -0.45749572],\n",
       "       [ 6.043573  , -0.24743582],\n",
       "       [ 6.0852394 , -0.5625439 ],\n",
       "       [ 6.1269064 , -0.5551361 ],\n",
       "       [ 6.168573  , -0.3077935 ],\n",
       "       [ 6.2102394 , -0.41389614],\n",
       "       [ 6.2519064 , -0.557226  ],\n",
       "       [ 6.293573  , -0.235037  ],\n",
       "       [ 6.3352394 , -0.41666666],\n",
       "       [ 6.3769064 , -0.5625439 ],\n",
       "       [ 6.418573  , -0.36115757],\n",
       "       [ 6.4602394 , -0.58138776],\n",
       "       [ 6.5019064 , -0.6430165 ],\n",
       "       [ 6.543573  , -0.28997615],\n",
       "       [ 6.5852394 , -0.50487816],\n",
       "       [ 6.6269064 , -0.584317  ],\n",
       "       [ 6.668573  , -0.5498414 ],\n",
       "       [ 6.7102394 , -0.34391797],\n",
       "       [ 6.7519064 , -0.48795003],\n",
       "       [ 6.793573  , -0.42640144],\n",
       "       [ 6.8352394 , -0.3485618 ],\n",
       "       [ 6.8769064 , -0.41053542],\n",
       "       [ 6.918573  , -0.41053542],\n",
       "       [ 6.9602394 , -0.28867513],\n",
       "       [ 7.0019064 , -0.503003  ],\n",
       "       [ 7.043573  , -0.3485618 ],\n",
       "       [ 7.0852394 ,  2.4358435 ],\n",
       "       [ 7.1269064 , -0.64289063],\n",
       "       [ 7.168573  , -0.43685204],\n",
       "       [ 7.2102394 , -0.32616404],\n",
       "       [ 7.2519064 , -0.4602873 ],\n",
       "       [ 7.293573  , -0.45749572],\n",
       "       [ 7.3352394 , -0.3077935 ],\n",
       "       [ 7.3769064 , -0.44500026],\n",
       "       [ 7.418573  , -0.38186362],\n",
       "       [ 7.4602394 , -0.3077935 ],\n",
       "       [ 7.5019064 , -0.47543445],\n",
       "       [ 7.543573  , -0.4602873 ],\n",
       "       [ 7.5852394 , -0.34391797],\n",
       "       [ 7.6269064 , -0.42640144],\n",
       "       [ 7.668573  , -0.42640144],\n",
       "       [ 7.7102394 , -0.26863533],\n",
       "       [ 7.7519064 , -0.47543445],\n",
       "       [ 7.793573  , -0.37796447],\n",
       "       [ 7.8352394 , -0.24743582],\n",
       "       [ 7.8769064 , -0.48795003],\n",
       "       [ 7.918573  , -0.37463433],\n",
       "       [ 7.9602394 , -0.26863533],\n",
       "       [ 8.001906  , -0.47543445],\n",
       "       [ 8.043572  , -0.34391797],\n",
       "       [ 8.085239  , -0.24743582],\n",
       "       [ 8.126906  , -0.47278896],\n",
       "       [ 8.168572  , -0.41053542],\n",
       "       [ 8.210239  , -0.31351885],\n",
       "       [ 8.251906  , -0.5477226 ],\n",
       "       [ 8.293572  , -0.42640144],\n",
       "       [ 8.335239  , -0.2247333 ],\n",
       "       [ 8.376906  , -0.45749572],\n",
       "       [ 8.418572  , -0.41053542],\n",
       "       [ 8.460239  , -0.26863533],\n",
       "       [ 8.501906  , -0.3944053 ],\n",
       "       [ 8.543572  , -0.37796447],\n",
       "       [ 8.585239  , -0.26863533],\n",
       "       [ 8.626906  ,  1.4734234 ],\n",
       "       [ 8.668572  , -0.41389614],\n",
       "       [ 8.710239  , -0.32616404],\n",
       "       [ 8.751906  , -0.49656352],\n",
       "       [ 8.793572  , -0.33129457],\n",
       "       [ 8.835239  , -0.28867513],\n",
       "       [ 8.876906  , -0.503003  ],\n",
       "       [ 8.918572  , -0.32254297],\n",
       "       [ 8.960239  , -0.2247333 ],\n",
       "       [ 9.001906  , -0.42640144],\n",
       "       [ 9.043572  , -0.36115757],\n",
       "       [ 9.085239  , -0.41053542],\n",
       "       [ 9.126906  , -0.26863533],\n",
       "       [ 9.168572  , -0.44204333],\n",
       "       [ 9.210239  , -0.44647622],\n",
       "       [ 9.251906  , -0.3077935 ],\n",
       "       [ 9.293572  , -0.48795003],\n",
       "       [ 9.335239  , -0.39801487],\n",
       "       [ 9.376906  , -0.26863533],\n",
       "       [ 9.418572  , -0.44204333],\n",
       "       [ 9.460239  , -0.42954656],\n",
       "       [ 9.501906  , -0.1723455 ],\n",
       "       [ 9.543572  , -0.36115757],\n",
       "       [ 9.585239  , -0.41389614],\n",
       "       [ 9.626906  , -0.32616404],\n",
       "       [ 9.668572  ,  2.189082  ],\n",
       "       [ 9.710239  ,  1.9880596 ],\n",
       "       [ 9.751906  , -0.2247333 ],\n",
       "       [ 9.793572  , -0.32616404],\n",
       "       [ 9.835239  , -0.44204333],\n",
       "       [ 9.876906  , -0.2       ],\n",
       "       [ 9.918572  , -0.37463433],\n",
       "       [ 9.960239  , -0.5179698 ],\n",
       "       [10.001906  , -0.14002801],\n",
       "       [10.043572  , -0.37796447],\n",
       "       [10.085239  , -0.34391797],\n",
       "       [10.126906  , -0.2247333 ],\n",
       "       [10.168572  , -0.37796447],\n",
       "       [10.210239  ,  2.4358435 ],\n",
       "       [10.251906  , -0.28867513],\n",
       "       [10.293572  , -0.41389614],\n",
       "       [10.335239  , -0.42640144],\n",
       "       [10.376906  , -0.28867513],\n",
       "       [10.418572  , -0.41053542],\n",
       "       [10.460239  , -0.36539704],\n",
       "       [10.501906  , -0.24743582],\n",
       "       [10.543572  , -0.36115757],\n",
       "       [10.585239  , -0.3944053 ],\n",
       "       [10.626906  , -0.3077935 ],\n",
       "       [10.668572  , -0.36539704],\n",
       "       [10.710239  , -0.4602873 ],\n",
       "       [10.751906  , -0.3077935 ],\n",
       "       [10.793572  , -0.4015379 ],\n",
       "       [10.835239  , -0.47543445],\n",
       "       [10.876906  ,  2.348981  ],\n",
       "       [10.918572  , -0.3944053 ],\n",
       "       [10.960239  , -0.37796447],\n",
       "       [11.001906  , -0.1723455 ],\n",
       "       [11.043572  , -0.36539704],\n",
       "       [11.085239  , -0.37796447],\n",
       "       [11.126906  , -0.14002801],\n",
       "       [11.168572  ,  1.6495242 ],\n",
       "       [11.210239  ,  2.7688746 ],\n",
       "       [11.251906  , -0.24743582],\n",
       "       [11.293572  , -0.3485618 ],\n",
       "       [11.335239  , -0.5259237 ],\n",
       "       [11.376906  ,  3.2569413 ],\n",
       "       [11.418572  , -0.34391797],\n",
       "       [11.460239  , -0.36115757],\n",
       "       [11.501906  , -0.28867513],\n",
       "       [11.543572  , -0.2247333 ],\n",
       "       [11.585239  , -0.34391797],\n",
       "       [11.626906  , -0.46692398],\n",
       "       [11.668572  , -0.39801487],\n",
       "       [11.710239  , -0.38186362],\n",
       "       [11.751906  , -0.47543445],\n",
       "       [11.793572  , -0.24743582],\n",
       "       [11.835239  ,  1.9332067 ],\n",
       "       [11.876906  , -0.37796447],\n",
       "       [11.918572  , -0.38186362],\n",
       "       [11.960239  , -0.41666666],\n",
       "       [12.001906  ,  2.052278  ],\n",
       "       [12.043572  , -0.32616404],\n",
       "       [12.085239  , -0.36115757],\n",
       "       [12.126906  , -0.39801487],\n",
       "       [12.168572  , -0.32616404],\n",
       "       [12.210239  , -0.46120068],\n",
       "       [12.251906  , -0.43685204],\n",
       "       [12.293572  , -0.34391797],\n",
       "       [12.335239  , -0.4138383 ],\n",
       "       [12.376906  ,  1.5033548 ],\n",
       "       [12.418572  , -0.3944053 ],\n",
       "       [12.460239  , -0.4519527 ],\n",
       "       [12.501906  , -0.49038464],\n",
       "       [12.543572  , -0.3485618 ],\n",
       "       [12.585239  ,  1.8860967 ],\n",
       "       [12.626906  , -0.5350731 ],\n",
       "       [12.668572  , -0.36539704],\n",
       "       [12.710239  , -0.31351885],\n",
       "       [12.751906  , -0.47186613],\n",
       "       [12.793572  , -0.37796447],\n",
       "       [12.835239  , -0.44647622],\n",
       "       [12.876906  , -0.35846618],\n",
       "       [12.918572  , -0.3077935 ],\n",
       "       [12.960239  , -0.36115757],\n",
       "       [13.001906  , -0.3485618 ],\n",
       "       [13.043572  , -0.34391797],\n",
       "       [13.085239  ,  2.5354629 ],\n",
       "       [13.126906  , -0.34391797],\n",
       "       [13.168572  , -0.3077935 ],\n",
       "       [13.210239  , -0.3485618 ],\n",
       "       [13.251906  , -0.31351885],\n",
       "       [13.293572  , -0.2247333 ],\n",
       "       [13.335239  , -0.34391797],\n",
       "       [13.376906  , -0.32616404],\n",
       "       [13.418572  , -0.28867513],\n",
       "       [13.460239  , -0.32616404],\n",
       "       [13.501906  , -0.27604246],\n",
       "       [13.543572  , -0.26863533],\n",
       "       [13.585239  , -0.3944053 ],\n",
       "       [13.626906  , -0.28867513],\n",
       "       [13.668572  , -0.1723455 ],\n",
       "       [13.710239  , -0.3077935 ],\n",
       "       [13.751906  , -0.34391797],\n",
       "       [13.793572  , -0.2       ],\n",
       "       [13.835239  , -0.2247333 ],\n",
       "       [13.876906  , -0.32616404],\n",
       "       [13.918572  , -0.3944053 ],\n",
       "       [13.960239  , -0.2       ],\n",
       "       [14.001906  , -0.24743582],\n",
       "       [14.043572  , -0.3077935 ],\n",
       "       [14.085239  , -0.34391797],\n",
       "       [14.126906  , -0.46692398],\n",
       "       [14.168572  , -0.43685204],\n",
       "       [14.210239  , -0.34391797],\n",
       "       [14.251906  , -0.41053542],\n",
       "       [14.293572  , -0.44204333],\n",
       "       [14.335239  , -0.2       ],\n",
       "       [14.376906  , -0.42954656],\n",
       "       [14.418572  , -0.39801487],\n",
       "       [14.460239  , -0.2       ],\n",
       "       [14.501906  , -0.47278896],\n",
       "       [14.543572  , -0.42954656],\n",
       "       [14.585239  , -0.32616404],\n",
       "       [14.626906  , -0.39052093],\n",
       "       [14.668572  , -0.44500026],\n",
       "       [14.710239  , -0.27604246],\n",
       "       [14.751906  , -0.3380617 ],\n",
       "       [14.793572  , -0.3485618 ],\n",
       "       [14.835239  , -0.1723455 ],\n",
       "       [14.876906  , -0.49656352],\n",
       "       [14.918572  , -0.3485618 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_loader = NeuralDatasetLoader()\n",
    "batch = neural_loader.get_batch()\n",
    "list(zip(batch[0],batch[1]))\n",
    "neural_loader.generate_datastack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06fbf9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.4450],\n",
      "        [-0.4575],\n",
      "        [-0.2247],\n",
      "        [-0.3078],\n",
      "        [-0.3135],\n",
      "        [-0.4880],\n",
      "        [ 2.0523],\n",
      "        [-0.2887],\n",
      "        [-0.4062],\n",
      "        [-0.3439]]), tensor([[-0.5572],\n",
      "        [-0.3078],\n",
      "        [-0.4420],\n",
      "        [-0.3313],\n",
      "        [-0.4603],\n",
      "        [-0.3780],\n",
      "        [-0.3612],\n",
      "        [-0.4105],\n",
      "        [-0.3585],\n",
      "        [-0.3439]]), tensor([[-0.6088],\n",
      "        [-0.2247],\n",
      "        [-0.5030],\n",
      "        [-0.3780],\n",
      "        [-0.3078],\n",
      "        [-0.3992],\n",
      "        [-0.2474],\n",
      "        [-0.3486],\n",
      "        [-0.4167],\n",
      "        [ 1.4734]]), tensor([[-0.4167],\n",
      "        [-0.2887],\n",
      "        [-0.4543],\n",
      "        [-0.3780],\n",
      "        [-0.4105],\n",
      "        [-0.3819],\n",
      "        [-0.4264],\n",
      "        [-0.4062],\n",
      "        [-0.3439],\n",
      "        [-0.2474]]), tensor([[-0.4105],\n",
      "        [-0.2887],\n",
      "        [-0.4818],\n",
      "        [-0.4139],\n",
      "        [-0.3262],\n",
      "        [-0.3944],\n",
      "        [-0.3944],\n",
      "        [-0.4420],\n",
      "        [-0.4139],\n",
      "        [-0.4139]]), tensor([[-0.4575],\n",
      "        [-0.2247],\n",
      "        [-0.4264],\n",
      "        [-0.3439],\n",
      "        [-0.3313],\n",
      "        [-0.2247],\n",
      "        [-0.1723],\n",
      "        [-0.3990],\n",
      "        [-0.2887],\n",
      "        [-0.4369]]), tensor([[-0.3078],\n",
      "        [-0.4264],\n",
      "        [-0.2686],\n",
      "        [-0.4139],\n",
      "        [-0.2474],\n",
      "        [ 3.2569],\n",
      "        [-0.4465],\n",
      "        [-0.5814],\n",
      "        [-0.2887],\n",
      "        [-0.2350]]), tensor([[-0.2247],\n",
      "        [-0.1400],\n",
      "        [-0.3654],\n",
      "        [-0.3944],\n",
      "        [-0.3746],\n",
      "        [-0.4905],\n",
      "        [-0.4139],\n",
      "        [-0.4264],\n",
      "        [-0.2474],\n",
      "        [-0.4754]]), tensor([[-0.3654],\n",
      "        [-0.4754],\n",
      "        [-0.4295],\n",
      "        [-0.2686],\n",
      "        [-0.2474],\n",
      "        [ 2.4358],\n",
      "        [-0.2474],\n",
      "        [-0.3078],\n",
      "        [-0.2000],\n",
      "        [-0.3262]]), tensor([[-0.1400],\n",
      "        [-0.3612],\n",
      "        [-0.5625],\n",
      "        [-0.3654],\n",
      "        [-0.4420],\n",
      "        [-0.3612],\n",
      "        [-0.3819],\n",
      "        [-0.3420],\n",
      "        [-0.3980],\n",
      "        [-0.3225]]), tensor([[-0.2247],\n",
      "        [-0.4138],\n",
      "        [-0.5498],\n",
      "        [-0.2474],\n",
      "        [ 2.7689],\n",
      "        [-0.3780],\n",
      "        [-0.5113],\n",
      "        [-0.4880],\n",
      "        [-0.4905],\n",
      "        [-0.3980]]), tensor([[-0.4575],\n",
      "        [-0.3612],\n",
      "        [-0.2686],\n",
      "        [-0.3439],\n",
      "        [-0.4880],\n",
      "        [-0.3078],\n",
      "        [-0.4754],\n",
      "        [-0.4105],\n",
      "        [-0.3819],\n",
      "        [-0.4420]]), tensor([[-0.5843],\n",
      "        [-0.3439],\n",
      "        [-0.2686],\n",
      "        [-0.3439],\n",
      "        [-0.3707],\n",
      "        [-0.2000],\n",
      "        [-0.2686],\n",
      "        [-0.3780],\n",
      "        [-0.3780],\n",
      "        [-0.2900]]), tensor([[ 2.3490],\n",
      "        [-0.3439],\n",
      "        [-0.2686],\n",
      "        [-0.3486],\n",
      "        [-0.3486],\n",
      "        [-0.3841],\n",
      "        [-0.3262],\n",
      "        [-0.3944],\n",
      "        [-0.2887],\n",
      "        [-0.4966]]), tensor([[-0.2887],\n",
      "        [-0.4264],\n",
      "        [-0.3262],\n",
      "        [-0.4975],\n",
      "        [-0.3439],\n",
      "        [-0.4450],\n",
      "        [-0.3944],\n",
      "        [-0.2887],\n",
      "        [-0.4603],\n",
      "        [-0.4264]]), tensor([[-0.2686],\n",
      "        [-0.4728],\n",
      "        [-0.3262],\n",
      "        [-0.3654],\n",
      "        [-0.3612],\n",
      "        [-0.2887],\n",
      "        [ 2.4358],\n",
      "        [-0.3135],\n",
      "        [-0.4105],\n",
      "        [-0.2000]]), tensor([[-0.4520],\n",
      "        [-0.5481],\n",
      "        [-0.3980],\n",
      "        [-0.5625],\n",
      "        [-0.4905],\n",
      "        [-0.3612],\n",
      "        [-0.3612],\n",
      "        [-0.3439],\n",
      "        [-0.3262],\n",
      "        [-0.3078]]), tensor([[ 2.1891],\n",
      "        [ 1.8861],\n",
      "        [-0.3819],\n",
      "        [-0.4669],\n",
      "        [-0.3225],\n",
      "        [-0.4612],\n",
      "        [-0.4819],\n",
      "        [-0.6429],\n",
      "        [-0.2474],\n",
      "        [-0.3439]]), tensor([[-0.6430],\n",
      "        [-0.3780],\n",
      "        [-0.4420],\n",
      "        [-0.2474],\n",
      "        [-0.2686],\n",
      "        [-0.5430],\n",
      "        [ 1.9881],\n",
      "        [-0.3585],\n",
      "        [-0.4728],\n",
      "        [ 1.6495]]), tensor([[-0.1400],\n",
      "        [-0.4754],\n",
      "        [-0.4603],\n",
      "        [-0.3905],\n",
      "        [-0.3780],\n",
      "        [-0.2474],\n",
      "        [-0.3078],\n",
      "        [-0.3262],\n",
      "        [-0.4062],\n",
      "        [-0.3135]]), tensor([[-0.4139],\n",
      "        [-0.5477],\n",
      "        [-0.4295],\n",
      "        [-0.5030],\n",
      "        [-0.3780],\n",
      "        [-0.3486],\n",
      "        [-0.3746],\n",
      "        [-0.3135],\n",
      "        [-0.3486],\n",
      "        [-0.5551]]), tensor([[-0.4105],\n",
      "        [-0.3262],\n",
      "        [-0.3819],\n",
      "        [-0.4719],\n",
      "        [-0.3612],\n",
      "        [-0.3746],\n",
      "        [-0.4754],\n",
      "        [-0.4905],\n",
      "        [-0.3439],\n",
      "        [-0.3612]]), tensor([[-0.3780],\n",
      "        [-0.3078],\n",
      "        [-0.4167],\n",
      "        [-0.5054],\n",
      "        [-0.4167],\n",
      "        [-0.3078],\n",
      "        [-0.4603],\n",
      "        [-0.4754],\n",
      "        [-0.3585],\n",
      "        [-0.5180]]), tensor([[-0.3078],\n",
      "        [-0.4754],\n",
      "        [-0.5351],\n",
      "        [-0.4754],\n",
      "        [-0.3780],\n",
      "        [-0.4603],\n",
      "        [ 2.5355],\n",
      "        [-0.3439],\n",
      "        [-0.2247],\n",
      "        [-0.3486]]), tensor([[-0.2247],\n",
      "        [-0.4105],\n",
      "        [-0.5049],\n",
      "        [-0.5351],\n",
      "        [-0.4015],\n",
      "        [-0.4966],\n",
      "        [-0.2474],\n",
      "        [-0.1400],\n",
      "        [-0.3135],\n",
      "        [-0.4619]]), tensor([[-0.4264],\n",
      "        [-0.3612],\n",
      "        [ 1.9332],\n",
      "        [-0.3078],\n",
      "        [-0.3439],\n",
      "        [-0.3585],\n",
      "        [-0.4264],\n",
      "        [-0.3439],\n",
      "        [ 1.5034],\n",
      "        [-0.4465]]), tensor([[-0.3980],\n",
      "        [-0.5551],\n",
      "        [-0.4369],\n",
      "        [-0.3780],\n",
      "        [-0.3078],\n",
      "        [-0.2887],\n",
      "        [-0.3944],\n",
      "        [-0.2000],\n",
      "        [-0.3439],\n",
      "        [-0.5646]])]\n"
     ]
    }
   ],
   "source": [
    "all_batches = []\n",
    "all_batch_labels = []\n",
    "while True:  # or some condition to end the loop\n",
    "    batch, batch_labels = neural_loader.get_batch()\n",
    "    if batch is None or batch_labels is None:\n",
    "        break  # Exit loop if there's no more data\n",
    "\n",
    "\n",
    "\n",
    "    all_batches.append(batch)\n",
    "    all_batch_labels.append(batch_labels)\n",
    "\n",
    "# Now, all_batches and all_batch_labels are lists of tensors, each representing a batch\n",
    "# Example: iterate through each batch\n",
    "dataloader = zip(all_batches,all_batch_labels)\n",
    "print(all_batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25bc1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fn, _ = get_transforms(nggp_model.cnf, nggp_model.use_conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7a4bdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.79357288,   1.83523954,   1.87690621, ..., 896.44249413,\n",
       "       896.4841608 , 896.52582746])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/jessegill/Desktop/nggp/data/ST260_Day1.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = all_batches[0]\n",
    "test_labels = all_batch_labels[0]\n",
    "\n",
    "test_x = torch.linspace(0, 15, 3).unsqueeze(0).T\n",
    "\n",
    "test_x = data['time'][:40]\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32).unsqueeze(1)  # reshape to (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2df2aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of test_x: torch.Size([40, 1])\n",
      "Reshaped test_x: torch.Size([40, 1])\n",
      "tensor([[-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749],\n",
      "        [-0.4749]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_x = torch.tensor(np.linspace(0,10,40), dtype=torch.float32).unsqueeze(1)\n",
    "nggp_model.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "print(\"Original shape of test_x:\", test_x.shape)\n",
    "\n",
    "# If your model expects an input shape of [batch_size, N], reshape test_x accordingly\n",
    "# For example, if N is 40:\n",
    "#test_x = test_x.view(-1, 40)  # Reshape to [batch_size, 40]\n",
    "\n",
    "print(\"Reshaped test_x:\", test_x.shape)\n",
    "\n",
    "# Continue with the model prediction\n",
    "z = nggp_model.feature_extractor(test_x)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    predictions = nggp_model.model(z)\n",
    "    means = sample_fn(predictions.mean.unsqueeze(1))\n",
    "\n",
    "\n",
    "print(means)\n",
    "# Plot the mean predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_x, means)\n",
    "plt.title(\"Mean Prediction of Gaussian Process\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Mean Prediction\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
